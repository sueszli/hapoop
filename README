when working with big data you can't just load everything from storage into memory and process it. you have to split the data into chunks and process them in parallel. This is called data parallelism. Hadoop and MapReduce are examples of data parallelism.

don't mistake them for task parallelism where you share compute.

most hadoop tasks are IO bound, not CPU bound - they rarely utilize the CPU to its full potential.

therefore the number of mappers and reducers you instantiate should be based on the number of cores you have, not the number of nodes.

if you have too many reduce tasks (ie. don't use combiners to merge intermediate results) you will have a lot of network calls.

network calls cause higher latency than disk IO.

in fact, if you look at the history of computing, the latency of network has always progressed significantly slower than the latency of disk io. additionally if you have a lot of network calls, you will have a lot of network congestion.

this is why RDMA and Infiniband are so important for big data. it's the greatest bottleneck.

don't use Hadoop for small data. it's not worth it. use a database.

ie. in this example were processing ~50GB of data from the amazon reviews dataset on a server with over ~100GB of RAM. we could have loaded the entire dataset into memory and processed it in a few seconds. but we're using Hadoop for the sake of learning.

having said that - it also becomes clear why we have reduced the number of mappers and reducers to 1. we don't have enough data to justify more than 1 mapper and 1 reducer. but in a real world scenario, you would want to scale accordingly.
